# Clustering-using-K-Means-and-Bradley-Fayyad-Reina-BFR-algorithm
 Implementation of K-Means and Bradley-Fayyad-Reina (BFR) algorithm from scratch without using any library.
 
# **Introduction:**

## **Clustering:** 
  
  â—† Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups        are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups      with similar traits and assign them into clusters.
  
  â—† Clustering is an unsupervised learning algorithm and an important task, that help us find more useful insights from data as we are      aware that most of the data in the real-world scenario is unlabelled.
  
  â—† We are aware about the K-means clustering algorithm for the clustering of the data points but it is not productive for the large       datasets as it takes more memory and time. Thats why Bradley-Fayyad-Reina Algorithm came into existence.
  
  

## **BFR [Bradley-Fayyad-Reina]:**
  
  â—† The BFR algorithm, named after its inventors Bradley, Fayyad and Reina, is a variant of k-means algorithm that is designed to           cluster data in a high-dimensional Euclidean space. 
  
  â—† It is	a	variant	of	k-means	designed	to handle	very	large (disk-resident)	data	sets. However, K-means is used as an main memory         algorithm for generating initial clustering of data points. By making use of Bradley-Fayyad-Reina Algorithm there is a drastic           reduction in memory consumed and time taken for clustering of data points.

# **Dataset Description:**

The datasets used are synthetic datasets. Since the BFR algorithm has a strong assumption that the clusters are normally distributed with independent dimensions, datasets are generated by initializing some random centroids and creating data points with these centroids and some standard deviations to form the clusters. In this there are also some added data points as outliers. The â€œclusterâ€ number of these outliers is represented by -1 (i.e., no clusters). Figure 1 shows an example of the data points (in CSV format). The first column is the data point index. The rest columns represent the features/dimensions of the data point.

The coded clustering algorithm is tested on ten different types of testcase datasets which vary by many characteristics(some has more number of datapoints(count of millions), more number of feautures(>100), more number of outliers(hundreds of outliers) and **the code gave us an NMI of more than 0.9 for most of the cases and ran for less than 200 seconds on all the cases**. 

# **Approach:**

* Firstly, we will be using K-means clustering technique for initialization of clusters on a single chunk of file(main-memory algorithm).
  ## **K-Means++ Algorithm:** 
  * But as we are aware that the dataset consists of outliers, and K-means is highly sensitive to outliers.
  * In K-means, we will be randomly initialising centroids for clustering of data points, the probability of choosing an outlier as a       centroid is higher in some test cases which leaves a cluster with a single data point after iterations on first chunk.
  * In order to avoid this, we used Kmeans++ algorithm for initialization of clusters, where the outliers are removed from dataset by       calculating euclidean distance with less number of iterations keeping run time in mind.
  * Now the filtered datapoints are sent to K-means algorithm for initialization of clusters using only first chunk of data points.

* Now the BFR algorithm comes into play which is nothing but an extension of K-means algorithm.   
  ## **Bradley-Fayyad-Reina Algorithm**
  * Load the data points from one file.
  * Run K-Means on a small random sample of the data points to initialize the K centroids using the Euclidean distance as the                similarity measurement.
  * Use the K-Means result from b to generate the DS clusters (i.e., discard points and generate statistics).
  * The initialization of DS has finished, so far, you have K clusters in DS.
  * Run K-Means on the rest of the data points with a large number of clusters (e.g., 5 times of K) to generate CS (clusters with more       than one points) and RS (clusters with only one point).
  * Load the data points from next file.
  * For the new points, compare them to the clusters in DS using the Mahalanobis Distance and assign them to the nearest DS cluster if       the distance is < ð›¼âˆšð‘‘.
  * For the new points that are not assigned to DS clusters, using the Mahalanobis Distance and assign the points to the nearest CS         cluster if the distance is < ð›¼âˆšð‘‘.
  * For the new points that are not assigned to any clusters in DS or CS, assign them to RS.
  * Merge the data points in RS by running K-Means with a large number of clusters (e.g., 5 times of K) to generate CS (clusters with       more than one points) and RS (clusters with only one point).
  * Merge clusters in CS that have a Mahalanobis Distance < ð›¼âˆšð‘‘.
  * Repeat the steps f â€“ k until all the files are processed.
  * If this is the last round (after processing the last chunk of data points), merge clusters in CS with the clusters in DS that have a     Mahalanobis Distance < ð›¼âˆšð‘‘.
    (ð›¼ is a hyper-parameter, you can choose it to be around 2, 3 or 4).

* For this algorithm the metric used is NMI(Normalized Mutual Information) which is appropriate between clusters.

## **Normalized Mutual Information:**

Normalized Mutual Information (NMI) is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is normalized by some generalized mean of H(labels_true) and H(labels_pred)), defined by the average_method.

This measure is not adjusted for chance. Therefore adjusted_mutual_info_score might be preferred.This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values wonâ€™t change the score value in any way.

This metric is furthermore symmetric: switching label_true with label_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.

(Source: Scikit learn page)


# **Results:**

The following are the results of algorithm on ten different cases of test dataset. 

========== Case 1 ==========

Case 1, Normalized_mutual_info_score: 1.0.

========== Case 2 ==========

Case 2, Normalized_mutual_info_score: 0.9991522761123472.

========== Case 3 ==========

Case 3, Normalized_mutual_info_score: 0.9166314935216457.

========== Case 4 ==========

Case 4, Normalized_mutual_info_score: 0.8391938249688761.

========== Case 5 ==========

Case 5, Normalized_mutual_info_score: 0.8556091361909601.

========== Case 6 ==========

Case 6, Normalized_mutual_info_score: 0.955139581584529.

========== Case 7 ==========

Case 7, Normalized_mutual_info_score: 0.9104962118781372.

========== Case 8 ==========

Case 8, Normalized_mutual_info_score: 0.9563179222250597.

========== Case 9 ==========

Case 9, Normalized_mutual_info_score: 0.9999083000156554.

========== Case 10 ==========

Case 10, Normalized_mutual_info_score: 0.9694457605128238.


# **References:**

Special thanks to Prof. Anna Farzindar and Prof. Yao-Yi chiang at University of Southern California for helping me in getting a clear understanding of the BFR algorithm both theoritically and practically.






 
